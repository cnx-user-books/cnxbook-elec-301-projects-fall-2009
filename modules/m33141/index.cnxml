<document xmlns="http://cnx.rice.edu/cnxml" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:bib="http://bibtexml.sf.net/">
  <title>Results, Conclusions, and Future Work</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m33141</md:content-id>
  <md:title>Results, Conclusions, and Future Work</md:title>
  <md:abstract>This module is part of a collection of modules for a class project on matrix completion techniques for the sensor network localization problem done for the Fall, 2009 offering of Prof.  Baraniuk's ELEC 301 course at Rice University.</md:abstract>
  <md:uuid>5105091c-70af-457b-a713-155639b21104</md:uuid>
</metadata>
<content>
    <section id="cid1">
      <title>Results, Conclusions, and Future Work</title>
      <section id="uid1">
        <title>Random Knock-Out Trials</title>
        <para id="id2253188">The results from the simulations for the random knock-out runs are displayed in the figures below, which depict the average relative Frobenius-norm error over 25 trials versus fraction of unknown entries.</para>
        <figure id="uid2">
          <media id="uid2_media" alt="">
            <image mime-type="image/png" src="../../media/expt1A.png" id="uid2_onlineimage" width="561"><!-- NOTE: attribute width changes image size online (pixels). original width is 561. --></image>
            <image mime-type="application/postscript" for="pdf" src="../../media/expt1A.eps" id="uid2_printimage"/>
          </media>
          <caption>Simulation results for random knock-out trials with no noise.</caption>
        </figure>
        <figure id="uid3">
          <media id="uid3_media" alt="">
            <image mime-type="image/png" src="../../media/expt1B.png" id="uid3_onlineimage" width="561"><!-- NOTE: attribute width changes image size online (pixels). original width is 561. --></image>
            <image mime-type="application/postscript" for="pdf" src="../../media/expt1B.eps" id="uid3_printimage"/>
          </media>
          <caption>Simulation results for random knock-out trials with noise present.</caption>
        </figure>
        <para id="id2253847">As these two figures illustrate, the results for the random knock-out trials were quite good. As expected, as the fraction of unknown entries becomes large, the error eventually becomes severe, while for very low fractions of
unknown entries, the error is extremely small. What is amazing is that for moderate fractions of unknown entries the algorithm still performs remarkably well, and its performance doesn't degrade much by the loss of a few more entries:
the graphs are nearly flat over the range from 0.3 to 0.8! As the second figure shows (and as might be imagined), noise only makes the error worse; however, the plot also shows that the algorithm is reasonably robust to noise in
that perturbations of the distance data by small amounts of noise don't become magnified into massive errors.</para>
        <para id="id2253864">As an example, consider Figure 3 below, which displays the results of a typical no-noise random knock-out run with knock-out probability 0.5. On the left is a plot of the sparsity pattern for the incomplete matrix. A blue dot
represents a known entry, while a blank space represents an unknown one. On the right is a plot of what the network looks like after being reconstructed using multidimensional scaling. Observe that the red circles for the network
corresponding to the network generated by the completed matrix enclose the blue dots of the original network's structure quite well, indicating that the match is very good.</para>
        <figure id="uid4">
          <media id="uid4_media" alt="">
            <image mime-type="image/png" src="../../media/ex1.png" id="uid4_onlineimage" width="902"><!-- NOTE: attribute width changes image size online (pixels). original width is 902. --></image>
            <image mime-type="application/postscript" for="pdf" src="../../media/ex1.eps" id="uid4_printimage"/>
          </media>
          <caption>Results from a typical no-noise random knock-out trial with a knock-out probability of 0.5. Left: Sparsity pattern for the incomplete matrix. Right: Overlay figure demonstrating degree of agreement between the original
network and the network generated from the completed matrix.</caption>
        </figure>
        <para id="id2253896">For an illustration of how the results look with noise, see Figure 4 below. This figure shows the results of a typical noise-present random knock-out run with knock-out probability of 0.5 and noise standard deviation 0.05. The
agreement in the reconstructed network is not as good as it was for the no-noise case, but the points of the reconstructed network are “clustered" in the right locations, and some of the prominent features of the original network
are present in the reconstructed one as well.</para>
        <figure id="uid5">
          <media id="uid5_media" alt="">
            <image mime-type="image/png" src="../../media/ex2.png" id="uid5_onlineimage" width="902"><!-- NOTE: attribute width changes image size online (pixels). original width is 902. --></image>
            <image mime-type="application/postscript" for="pdf" src="../../media/ex2.eps" id="uid5_printimage"/>
          </media>
          <caption>Results from a typical noise-present random knock-out trial with a knock-out probability of 0.5 and a noise standard deviation of 0.05. Left: Sparsity pattern for the incomplete matrix. Right: Overlay figure demonstrating
degree of agreement between the original network and the network generated from the completed matrix.</caption>
        </figure>
      </section>
      <section id="uid6">
        <title>Realistic Knock-Out Trials</title>
        <para id="id2253942">The figures below, which show the results for the realistic knock-out trials, are similar to those above except that they plot the average relative Frobenius-norm error over 25 trials versus maximum radius as opposed to fraction of
unknown entries.</para>
        <figure id="uid7">
          <media id="uid7_media" alt="">
            <image mime-type="image/png" src="../../media/expt2A.png" id="uid7_onlineimage" width="561"><!-- NOTE: attribute width changes image size online (pixels). original width is 561. --></image>
            <image mime-type="application/postscript" for="pdf" src="../../media/expt2A.eps" id="uid7_printimage"/>
          </media>
          <caption>Simulation results for realistic knock-out trials without noise.</caption>
        </figure>
        <figure id="uid8">
          <media id="uid8_media" alt="">
            <image mime-type="image/png" src="../../media/expt2B.png" id="uid8_onlineimage" width="561"><!-- NOTE: attribute width changes image size online (pixels). original width is 561. --></image>
            <image mime-type="application/postscript" for="pdf" src="../../media/expt2B.eps" id="uid8_printimage"/>
          </media>
          <caption>Simulation results for realistic knock-out trials with noise present.</caption>
        </figure>
        <para id="id2253976">The most salient feature of these graphs is the odd “hump" that appears from radius values of about 0.5 to 0.7, even in the no-noise case. Over this range, despite the fact that the radius is growing (meaning that more pairwise
distances are known), the error in the completed matrix is actually becoming <emphasis effect="italics">worse</emphasis> rather than better, which seems to contradict the excellent results discussed above for the random knock-out case. At the time of this writing,
we are still unsure as to why this “hump" appears; however, we suspect that it may have something to do with the OptSpace algorithm itself because when we run the same experiment using the SVT algorithm of Candès, the hump does
not appear, as the figure below shows. (Note that, nevertheless, OptSpace tends to produce less error than SVT, even over the offending range of radii.)</para>
        <figure id="uid9">
          <media id="uid9_media" alt="">
            <image mime-type="image/png" src="../../media/hump.png" id="uid9_onlineimage" width="561"><!-- NOTE: attribute width changes image size online (pixels). original width is 561. --></image>
            <image mime-type="application/postscript" for="pdf" src="../../media/hump.eps" id="uid9_printimage"/>
          </media>
          <caption>OptSpace performance vs. SVT performance for realistic entry knock-out without noise. SVT does not display a “hump," but OptSpace generally returns better error values.</caption>
        </figure>
        <para id="id2254028">Perhaps more important than the “hump," however, is the fact that the scales on the axes of the above graphs alone are enough to demonstrate that the performance of the method in the realistic knock-out case is decidedly worse than
that for the random knock-out case. For example, consider the figure below, which shows the results of a typical no-noise, realistick knock-out trial with a maximum radius of 1. For this particular trial, over 97 percent of the pairs are known. The reconstructed network matches the original quite well near the “center" of the network, but at the edges, the match becomes much worse. This behavior is not exhibited at all by random knock-out trials for
comparable fractions of unknown entries, as the picture at the bottom of the figure illustrates, which was generated from a non-noise random knock-out trial in which 90 percent of the pairs were known.</para>
        <figure id="uid10">
          <media id="uid10_media" alt="">
            <image mime-type="image/png" src="../../media/ex3.png" id="uid10_onlineimage" width="902"><!-- NOTE: attribute width changes image size online (pixels). original width is 902. --></image>
            <image mime-type="application/postscript" for="pdf" src="../../media/ex3.eps" id="uid10_printimage"/>
          </media>
          <caption>Results from a typical non-noise realistic knock-out trial with a maximum radius of 1. Top-Left: Sparsity pattern for the incomplete matrix. Top-Right: Overlay figure demonstrating degree of agreement between the original
network and the network generated from the completed matrix. Bottom: Typical results from a non-noise random knock-out trial with knock out probability of 0.1 (90% of distance pairs are known).</caption>
        </figure>
        <para id="id2254080">Shrinking the radius only makes matters worse, as the next figure illustrates. The maximum radius here is <m:math overflow="scroll"><m:mrow><m:msqrt><m:mn>2</m:mn></m:msqrt><m:mo>/</m:mo><m:mn>2</m:mn></m:mrow></m:math>. Around 77 percent of the distance pairs are known, and yet the match is terrible.</para>
        <figure id="uid11">
          <media id="uid11_media" alt="">
            <image mime-type="image/png" src="../../media/ex4.png" id="uid11_onlineimage" width="902"><!-- NOTE: attribute width changes image size online (pixels). original width is 902. --></image>
            <image mime-type="application/postscript" for="pdf" src="../../media/ex4.eps" id="uid11_printimage"/>
          </media>
          <caption>Results from a typical non-noise realistic knock-out trial with a maximum radius of <m:math overflow="scroll"><m:mrow><m:msqrt><m:mn>2</m:mn></m:msqrt><m:mo>/</m:mo><m:mn>2</m:mn><m:mo>.</m:mo></m:mrow></m:math> Left: Sparsity pattern for the incomplete matrix. Right: Overlay figure demonstrating degree of agreement between the original
network and the network generated from the completed matrix.</caption>
        </figure>
        <para id="id2254138">Adding noise only makes the results even worse. At first glance, this behavior appears to be inexplicable; however examining the sparsity patterns of the incomplete matrices reveals an interesting fact: the entry knock-out in the realistic
case is far from being “random!" The sparsity patterns for the realistic knock-out matrices reveal clear patterns of lines in their knocked-out entries that are not present in those for random knock-out cases. This unintended
regularity of entry selection violates the assumption made in all of the matrix completion literature that the known entries are taken from a uniform sampling of the matrix, so it would seem that none of the theoretical results
that have been derived apply in this case.</para>
      </section>
      <section id="uid12">
        <title>Conclusions</title>
        <para id="id2254169">Our results show that matrix completion presents a viable means of approaching the sensor network localization problem under the assumption that the known pairs of distances come from a uniform sampling of the distance matrix. Under
these conditions, matrix completion provides excellent network reconstruction and is fairly robust to noise. Unfortunately, its performance in the more realistic case in which distance information will be excluded or included based
on a maximum possible distance over which two sensors can communicate leaves much to be desired.</para>
      </section>
      <section id="uid13">
        <title>Future Work</title>
        <para id="id2254191">With more time on this project, we would like to have explored the following questions further:</para>
        <list id="id2254196" display="block" list-type="bulleted">
          <item id="uid14">What is the true origin of the mysterious “hump?" If it really is due to OptSpace as the above seems to suggest, is there a way to modify the OptSpace algorithm to get it to go away?
</item>
          <item id="uid15">What is the fundamental reason that the realistic knock-out trials did not work? Is there a way to get them to work better? (Perhaps something like permuting the distance entries in the matrix around to make the sampling pattern
apparently more random would do the trick. If the permutations are stored somewhere, they can be undone after the matrix is completed if necessary.)
</item>
          <item id="uid16">The experiment worked pretty well in two dimensions, at least for the random knock-out case. Will three dimensions show results that are any different?
</item>
        </list>
      </section>
    </section>
  </content>
  <bib:file/>
</document>